#!/usr/bin/env python3

import argparse
from sourcepredictlib.forest import sourceforest
from sourcepredictlib.forest import sourcemap
from sourcepredictlib import utils
import os
import pandas as pd
import numpy as np
import warnings
import sys
from . import utils


def _get_args():
    '''This function parses and return arguments passed in'''
    parser = argparse.ArgumentParser(
        prog='SourcePredict v' + str(version),
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=f'''
==========================================================
SourcePredict v{version}
Coprolite source classification
Author: Maxime Borry
Contact: <borry[at]shh.mpg.de>
Homepage & Documentation: github.com/maxibor/sourcepredict
==========================================================
        ''')
    parser.add_argument('otu_table', help="path to otu table in csv format")
    parser.add_argument(
        '-a',
        dest="alpha",
        default=0.1,
        help="Proportion of sink sample in unknown. Default = 0.1")
    parser.add_argument(
        '-s',
        dest="sources",
        default=os.path.dirname(os.path.abspath(
            __file__))+'/data/modern_gut_microbiomes_sources.csv',
        help="Path to source csv file. Default = data/modern_gut_microbiomes_sources.csv")
    parser.add_argument(
        '-l',
        dest="labels",
        default=os.path.dirname(os.path.abspath(
            __file__)) + '/data/modern_gut_microbiomes_labels.csv',
        help="Path to labels csv file. Default = data/modern_gut_microbiomes_labels.csv")
    parser.add_argument(
        '-n',
        dest="normalization",
        default='GMPR',
        help="Normalization method (RLE | CLR | Subsample | GMPR). Default = GMPR")
    parser.add_argument(
        '-pd',
        dest="pca_dim",
        default=20,
        help="Number of PCA components to retain for dimension reduction"
    )
    parser.add_argument(
        '-me',
        dest="method",
        default='TSNE',
        help="Embedding Method. TSNE or UMAP. Default = TSNE"
    )
    parser.add_argument(
        '-di',
        dest="dim",
        default=2,
        help="Number of dimensions to retain for dimension reduction"
    )
    parser.add_argument(
        '-o',
        dest="output",
        default=None,
        help="Output file basename. Default = <sample_basename>.sourcepredict.csv")
    parser.add_argument(
        '-e',
        dest="embed",
        default=None,
        help="Output embedding csv file. Default = None")
    parser.add_argument(
        '-se',
        dest="seed",
        default=42,
        help="Seed for random generator. Default = 42")
    parser.add_argument(
        '-k',
        dest="kfold",
        default=2,
        help="Number of fold for K-fold cross validation in feature selection and parameter optimization. Default = 3")
    parser.add_argument(
        '-t',
        dest="threads",
        default=2,
        help="Number of threads for parallel processing. Default = 2")

    args = parser.parse_args()

    sink = args.otu_table
    alpha = float(args.alpha)
    normalization = args.normalization
    sources = args.sources
    labels = args.labels
    seed = int(args.seed)
    pdim = int(args.pca_dim)
    method = args.method
    dim = int(args.dim)
    output = args.output
    embed = args.embed
    kfold = int(args.kfold)
    threads = int(args.threads)

    return(sink, alpha, normalization, sources, labels, seed, pdim, method, dim, output, embed, kfold, threads)


if __name__ == "__main__":
    version = "0.21"
    warnings.filterwarnings("ignore")
    SINK, ALPHA, NORMALIZATION, SOURCES, LABELS, SEED, PDIM, METHOD, DIM, OUTPUT, EMBED_CSV, KFOLD, THREADS = _get_args()
    SEED = utils.check_gen_seed(SEED)
    np.random.seed(SEED)
    embed_method = utils.check_embed(METHOD)
    normalization = utils.check_norm(NORMALIZATION)
    sinks = utils.split_sinks(SINK)
    predictions = {}
    unifrac_rank = "species"
    samp_pred = {}
    print("Step 1: Checking for unknown proportion")
    for s in sinks:
        sample = ''.join(list(s.columns))
        samp_pred[sample] = {}
        print(f"  == Sample: {sample} ==")
        a = sourceforest(source=SOURCES, sink=s, labels=LABELS)
        print("\tAdding unknown")
        a.add_unknown(alpha=ALPHA, seed=SEED)
        print(f"\tNormalizing ({normalization})")
        a.normalize(method=normalization, threads=THREADS)
        print("\tFeature engineering")
        a.dim_reduction(PDIM)
        # a.select_features(cv=KFOLD, quantile=0.20, threads=THREADS)
        print("\tRandom forest machine learning")
        pred = a.rndForest(seed=SEED, threads=THREADS,
                           outfile=OUTPUT, kfold=KFOLD)
        samp_pred[sample]['unknown'] = pred[sample]['unknown']

    print("Step 2: Checking for source proportion")
    u = sourcemap(train=SOURCES, test=SINK, labels=LABELS,
                  norm_method=NORMALIZATION, threads=THREADS)
    print(f"\tComputing weighted unifrac distance on {unifrac_rank} rank")
    u.compute_distance(rank=unifrac_rank)
    print(f"\t{embed_method} embedding")
    u.embed(n_comp=DIM, method=embed_method, seed=SEED, out_csv=EMBED_CSV)
    print("\tKNN machine learning")
    umap_pred = u.knn_classification(
        kfold=KFOLD, threads=THREADS, seed=SEED)

    prediction = utils.account_unk(samp_pred=samp_pred, umap_pred=umap_pred)
    if OUTPUT is None:
        OUTPUT = f"{utils._get_basename(SINK)}.sourcepredict.csv"
    prediction.to_csv(OUTPUT)
    print(f"Sourcepredict result written to {OUTPUT}")
