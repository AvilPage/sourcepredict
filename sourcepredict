#!/usr/bin/env python3

import argparse
from sourcepredictlib.forest import sourceforest
from sourcepredictlib import utils
import os
import pandas as pd
import warnings
from . import utils


def _get_args():
    '''This function parses and return arguments passed in'''
    parser = argparse.ArgumentParser(
        prog='SourcePredict v' + str(version),
        formatter_class=argparse.RawDescriptionHelpFormatter,
        description=f'''
==========================================================
SourcePredict v{version}
Coprolite source classification
Author: Maxime Borry
Contact: <borry[at]shh.mpg.de>
Homepage & Documentation: github.com/maxibor/sourcepredict
==========================================================
        ''')
    parser.add_argument('otu_table', help="path to otu table in csv format")
    parser.add_argument(
        '-a',
        dest="alpha",
        default=0.1,
        help="Proportion of sink sample in unknown. Default = 0.1")
    parser.add_argument(
        '-s',
        dest="sources",
        default=os.path.dirname(os.path.abspath(
            __file__))+'/data/modern_gut_microbiomes_sources.csv',
        help="Path to source csv file. Default = data/modern_gut_microbiomes_sources.csv")
    parser.add_argument(
        '-l',
        dest="labels",
        default=os.path.dirname(os.path.abspath(
            __file__)) + '/data/modern_gut_microbiomes_labels.csv',
        help="Path to labels csv file. Default = data/modern_gut_microbiomes_labels.csv")
    parser.add_argument(
        '-n',
        dest="normalization",
        default='RLE',
        help="Normalization method (RLE | CLR | Subsample). Default = RLE")
    parser.add_argument(
        '-pd',
        dest="pca_dim",
        default=20,
        help="Number of PCA components to retain for dimension reduction"
    )
    parser.add_argument(
        '-ud',
        dest="umap_dim",
        default=2,
        help="Number of UMAP dimensions to retain for dimension reduction"
    )
    parser.add_argument(
        '-o',
        dest="output",
        default=None,
        help="Output file basename. Default = <sample_basename>.sourcepredict.csv")
    parser.add_argument(
        '-u',
        dest="umap",
        default=None,
        help="Umap embedding csv file. Default = None")
    parser.add_argument(
        '-se',
        dest="seed",
        default=42,
        help="Seed for random generator. Default = 42")
    parser.add_argument(
        '-k',
        dest="kfold",
        default=3,
        help="Number of fold for K-fold cross validation in feature selection and parameter optimization. Default = 3")
    parser.add_argument(
        '-t',
        dest="threads",
        default=2,
        help="Number of threads for parallel processing. Default = 2")

    args = parser.parse_args()

    sink = args.otu_table
    alpha = float(args.alpha)
    normalization = args.normalization
    sources = args.sources
    labels = args.labels
    seed = int(args.seed)
    pdim = int(args.pca_dim)
    udim = int(args.umap_dim)
    output = args.output
    umap = args.umap
    kfold = int(args.kfold)
    threads = int(args.threads)

    return(sink, alpha, normalization, sources, labels, seed, pdim, udim, output, umap, kfold, threads)


if __name__ == "__main__":
    version = "0.2"
    warnings.filterwarnings("ignore")
    SINK, ALPHA, NORMALIZATION, SOURCES, LABELS, SEED, PDIM, UDIM, OUTPUT, UMAP_CSV, KFOLD, THREADS = _get_args()
    SEED = utils.check_gen_seed(SEED)
    NORMALIZATION = utils.check_norm(NORMALIZATION)
    sinks = utils.split_sinks(SINK)
    predictions = {}
    unifrac_rank = "species"
    for s in sinks:
        print(f"== Sample: {''.join(list(s.columns))} ==")
        print("Step 1: Checking for unknown proportion")
        a = sourceforest(source=SOURCES, sink=s, labels=LABELS)
        print("\tAdding unknown")
        a.add_unknown(alpha=ALPHA)
        print("\tNormalizing")
        a.normalize(method=NORMALIZATION)
        print("\tFeature engineering")
        a.dim_reduction(PDIM)
        # a.select_features(cv=KFOLD, quantile=0.20, threads=THREADS)
        print("\tRandom forest machine learning")
        predicted_unk = a.rndForest(seed=SEED, threads=THREADS,
                                    outfile=OUTPUT, kfold=KFOLD)

        print("Step 2: Checking for source proportion")
        print(f"\tComputing weighted unifrac distance on {unifrac_rank} rank")
        a.compute_distance(rank=unifrac_rank)
        print("\tUMAP embedding")
        a.embed(n_comp=UDIM, umap_csv=UMAP_CSV)
        print("\tKNN machine learning")
        predicted_source = a.knn_classification(
            kfold=KFOLD, threads=THREADS, seed=SEED)
        print("============================\n")

        prediction = utils.account_unk(predicted_unk, predicted_source)
        predictions["".join(list(s.columns))] = prediction
if OUTPUT is None:
    OUTPUT = f"{utils._get_basename(SINK)}.sourcepredict.csv"
pd.DataFrame(predictions).to_csv(OUTPUT)
